{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2r6v3FciCZMhHHU9KYxRW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bendavidsteel/assembly-compiler/blob/master/rl_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbj4bASTAhCZ",
        "outputId": "969d8116-b825-4f4d-a34f-b59d1408b8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter==2.0.4+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-sparse==0.6.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-cluster==1.5.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-spline-conv==1.2.0+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==2.0.4+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-2.0.4%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 341kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-sparse==0.6.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-0.6.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==0.6.5+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==0.6.5+cu101) (1.18.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-cluster==1.5.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-1.5.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 1.9MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-spline-conv==1.2.0+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-1.2.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 555kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/67/6c0bce6b6e6bc806e25d996e46a686e5a11254d89257983265a988bb02ee/torch_geometric-1.6.1.tar.gz (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/78/edadb45c7f26f8fbb99da81feadb561c26bb0393b6c5d1ac200ecdc12d61/ase-3.20.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (50.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.1-cp36-none-any.whl size=308552 sha256=f347f42b4a5fd9d3e1b0d23ace1f3b8348b52922524cf81038543fea7b6bfb53\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/25/ea/3d71d2088dccc63214fa59259dcc598ded4150a5f8b41d84ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.20.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtqspZ44BGMu",
        "outputId": "43afcb7d-df46-411a-af46-6c30d84596f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import statistics\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric as geo\n",
        "import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fdjRdDVvYlX"
      },
      "source": [
        "An object representing our environment, mapping (state, action) pairs to their (next_state, reward) result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh-O73yTvN8z"
      },
      "source": [
        "Action = collections.namedtuple('Action', ('foreign', 'domestic'))\n",
        "NumpyData = collections.namedtuple('NumpyData', ('x', 'edge_index', 'edge_attr'))\n",
        "State = collections.namedtuple('State', ('initial', 'sequence', 'batch'))\n",
        "Transition = collections.namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "def to_numpy_data(data):\n",
        "    return NumpyData(x = data.x.detach().numpy(),\n",
        "                     edge_index = data.edge_index.detach().numpy(),\n",
        "                     edge_attr = data.edge_attr.detach().numpy())\n",
        "    \n",
        "def data_from_numpy(data):\n",
        "    return geo.data.Data(x = torch.from_numpy(data.x),\n",
        "                         edge_index = torch.from_numpy(data.edge_index),\n",
        "                         edge_attr = torch.from_numpy(data.edge_attr))\n",
        "\n",
        "\n",
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def reset(self, initial, batch):\n",
        "        self.initial = initial.numpy()\n",
        "        self.batch = batch.numpy()\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if not self.states:\n",
        "            self.states.append(to_numpy_data(transition.state))\n",
        "        self.states.append(to_numpy_data(transition.next_state))\n",
        "        self.actions.append(Action(foreign = transition.action.foreign.detach().numpy(),\n",
        "                                   domestic = transition.action.domestic.detach().numpy()))\n",
        "        self.rewards.append(transition.reward.detach().numpy())\n",
        "\n",
        "    def sample(self):\n",
        "        sample_idx = random.randint(max(0, len(self.states) - 2 - self.capacity), len(self.states) - 2)\n",
        "        return Transition(state = State(initial = torch.from_numpy(self.initial), sequence = [data_from_numpy(state) for state in self.states[:sample_idx + 1]], batch = torch.from_numpy(self.batch)),\n",
        "                          action = Action(foreign = torch.from_numpy(self.actions[sample_idx].foreign), domestic = torch.from_numpy(self.actions[sample_idx].domestic)),\n",
        "                          reward = torch.from_numpy(self.rewards[sample_idx]),\n",
        "                          next_state = State(initial = torch.from_numpy(self.initial), sequence = [data_from_numpy(state) for state in self.states[:sample_idx + 2]], batch = torch.from_numpy(self.batch)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)                 "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TufYnaM6Qr3z"
      },
      "source": [
        "class RecurGraphNet(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # initial trainable hidden state for lstm\n",
        "        self.lstm_h_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "        self.lstm_c_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_linear = torch.nn.Linear(lstm_layer_size, num_output_features)\n",
        "\n",
        "    def reset(self, initial):\n",
        "        self.initial = initial\n",
        "        self.new_seq = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        # create graph representation\n",
        "        graph_step = torch.nn.functional.relu(self.conv(input.x, input.edge_index, input.edge_attr))\n",
        "\n",
        "        # recurrent stage\n",
        "        # initial state of lstm is representation of target prior to this sequence\n",
        "        if self.new_seq:\n",
        "            self.new_seq = False\n",
        "            self.hs = self.lstm_h_s(self.initial).unsqueeze(0)\n",
        "            self.cs = self.lstm_c_s(self.initial).unsqueeze(0)\n",
        "\n",
        "        lstm_output, (self.hs, self.cs) = self.lstm(graph_step.unsqueeze(0), (self.hs, self.cs))\n",
        "\n",
        "        return self.final_linear(lstm_output.squeeze(0))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqBv2fPEB1dK"
      },
      "source": [
        "class NationEnvironment():\n",
        "    def __init__(self, num_countries, device):\n",
        "        root = os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization')\n",
        "        self.norm_stats = torch.load(os.path.join(root, \"dataset\", \"processed\", \"norm_stats.pt\"))\n",
        "        best_model = torch.load(os.path.join(root, 'best_model_recurrent.pkl'))\n",
        "\n",
        "        self.num_countries = num_countries\n",
        "        self.device = device\n",
        "\n",
        "        num_node_features = 2\n",
        "        num_edge_features = 7\n",
        "        num_output_features = 1\n",
        "        self.env_model = RecurGraphNet(num_node_features, num_edge_features, num_output_features).to(device)\n",
        "        self.env_model.load_state_dict(best_model)\n",
        "        self.reset()\n",
        "\n",
        "        self.num_foreign_actions = 5\n",
        "        self.num_domestic_actions = 4\n",
        "        \n",
        "    def reset(self):\n",
        "        self.initial_demo = torch.rand(self.num_countries, 1, dtype=torch.float32)\n",
        "        self.norm_initial_demo = (self.initial_demo - self.norm_stats[\"y_mean\"]) / self.norm_stats[\"y_std\"]\n",
        "\n",
        "        # start with up to 1 thousand gdp and 1 million pop\n",
        "        gdp = 1000000000 * torch.rand(self.num_countries, 1, dtype=torch.float32)\n",
        "        pop = 1000000 * torch.rand(self.num_countries, 1, dtype=torch.float32)\n",
        "        self.node_features = torch.cat([gdp,\n",
        "                                        pop], dim=1)\n",
        "\n",
        "        # establish country ally clusters\n",
        "        self.clusters = []\n",
        "        cluster_edges = []\n",
        "        num_clusters = self.num_countries // 10\n",
        "        for cluster_idx in range(num_clusters):\n",
        "            cluster = random.sample(list(range(self.num_countries)), random.randint(2, self.num_countries // 5))\n",
        "            self.clusters.append(cluster)\n",
        "            for edge in list(itertools.permutations(cluster, 2)):\n",
        "                cluster_edges.append(edge)\n",
        "\n",
        "        # starting with number of links on average anywhere between 1 and 5\n",
        "        num_edges = (self.num_countries * random.randint(1, 5)) + len(cluster_edges)\n",
        "        self.edge_indexes = torch.randint(self.num_countries, (2, num_edges), dtype=torch.long)\n",
        "\n",
        "        for idx in range(len(cluster_edges)):\n",
        "            self.edge_indexes[0, idx] = cluster_edges[idx][0]\n",
        "            self.edge_indexes[1, idx] = cluster_edges[idx][1]\n",
        "\n",
        "        # ensure no self links\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if self.edge_indexes[0,idx] == self.edge_indexes[1,idx]:\n",
        "                if self.edge_indexes[1,idx] == self.num_countries:\n",
        "                    self.edge_indexes[1,idx] -= 1\n",
        "                else:\n",
        "                    self.edge_indexes[1,idx] += 1\n",
        "\n",
        "        # ever col -> curr col\n",
        "        #          -> common language\n",
        "        ever_col = (torch.rand(num_edges, 1) > 0.98)\n",
        "        curr_col = ((torch.rand(num_edges, 1) > 0.5) * ever_col)\n",
        "        com_lang = ((torch.rand(num_edges, 1) > 0.9) | ((torch.rand(num_edges, 1) > 0.5) * ever_col))\n",
        "        # distance -> distance by sea\n",
        "        #          -> shared borders\n",
        "        #          -> trade\n",
        "        coor_dis = 15000 * torch.rand(num_edges, 1, dtype=torch.float32)\n",
        "        sea_dist = coor_dis * ((2.5 * torch.rand(num_edges, 1, dtype=torch.float32)) + 1)\n",
        "        trad_imp = coor_dis * 10000 * torch.rand(num_edges, 1, dtype=torch.float32)\n",
        "        shar_bor = (((coor_dis < 1000) * (torch.rand(num_edges, 1) > 0.5)) | ((coor_dis < 2000) * (torch.rand(num_edges, 1) > 0.7)) | ((coor_dis < 5000) * (torch.rand(num_edges, 1) > 0.9))).float()\n",
        "        # order of edge features is distance, ever a colony, common language, shared borders, distance by sea, current colony, imports\n",
        "        self.edge_features = torch.cat([coor_dis.float(),\n",
        "                                        ever_col.float(),\n",
        "                                        com_lang.float(),\n",
        "                                        shar_bor.float(),\n",
        "                                        sea_dist.float(),\n",
        "                                        curr_col.float(),\n",
        "                                        trad_imp.float()], dim=1)\n",
        "        \n",
        "        self.env_model.reset(self.norm_initial_demo)\n",
        "\n",
        "        self.create_normed_state()\n",
        "\n",
        "        \n",
        "    def establish_trade(self, agent_id, target_id):\n",
        "        # ensure no self links\n",
        "        if agent_id == target_id:\n",
        "            return\n",
        "\n",
        "        # origin country index comes first\n",
        "        trade_link = torch.tensor([target_id, agent_id]).view(2,1)\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if ((self.edge_indexes[0,idx] == trade_link[0,0]) and (self.edge_indexes[1,idx] == trade_link[1,0])):\n",
        "                # trade link already established\n",
        "                return\n",
        "\n",
        "        # create features for new link\n",
        "        ever_col = 0\n",
        "        curr_col = 0\n",
        "        com_lang = random.random() > 0.9\n",
        "        coor_dis = 15000 * random.random()\n",
        "        sea_dist = coor_dis * ((2.5 * random.random()) + 1)\n",
        "        trad_imp = coor_dis * 10000 * random.random()\n",
        "        shar_bor = ((coor_dis < 1000) * (random.random() > 0.5)) | ((coor_dis < 2000) * (random.random() > 0.7)) | ((coor_dis < 5000) * (random.random() > 0.9))\n",
        "        new_features = torch.tensor([coor_dis,\n",
        "                                     ever_col,\n",
        "                                     com_lang,\n",
        "                                     shar_bor,\n",
        "                                     sea_dist,\n",
        "                                     curr_col,\n",
        "                                     trad_imp]).view(1, 7)\n",
        "\n",
        "        self.edge_features = torch.cat((self.edge_features, new_features), dim=0)\n",
        "        self.edge_indexes = torch.cat((self.edge_indexes, trade_link), dim=1)\n",
        "\n",
        "    def increase_imports(self, agent_id, target_id):\n",
        "        self.scale_imports(agent_id, target_id, 1.05, 1.3)\n",
        "\n",
        "    def decrease_imports(self, agent_id, target_id):\n",
        "        self.scale_imports(agent_id, target_id, 0.7, 0.95)\n",
        "\n",
        "    def scale_imports(self, agent_id, target_id, lower_bound, upper_bound):\n",
        "        link_idx = -1\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) and (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "                break\n",
        "\n",
        "        if link_idx == -1:\n",
        "            return\n",
        "        \n",
        "        self.edge_features[idx, 6] = self.edge_features[idx, 6] * random.uniform(lower_bound, upper_bound)\n",
        "\n",
        "    def colonize(self, agent_id, target_id):\n",
        "        # check if there is a link with this country\n",
        "        # and ensure no other country has already colonized the target country\n",
        "        link_idx = -1\n",
        "        already_colonised = False\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) and (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "\n",
        "            if self.edge_indexes[0,idx] == target_id:\n",
        "                if self.edge_features[idx, 5] == 1:\n",
        "                    already_colonised = True\n",
        "\n",
        "        if link_idx == -1 | already_colonised:\n",
        "            return\n",
        "\n",
        "        # colonizing country needs to be bigger\n",
        "        if (self.node_features[agent_id, 0] > 1.2 * self.node_features[target_id, 0]) and \\\n",
        "           (self.node_features[agent_id, 1] > 1.1 * self.node_features[target_id, 1]):\n",
        "            self.edge_features[link_idx, 5] = 1\n",
        "            self.edge_features[link_idx, 1] = 1\n",
        "\n",
        "    def decolonize(self, agent_id, target_id):\n",
        "        # check if there is a link with this country\n",
        "        link_idx = -1\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) and (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "                break\n",
        "\n",
        "        if link_idx == -1:\n",
        "            return\n",
        "\n",
        "        if self.edge_features[link_idx, 5] == 1:\n",
        "            self.edge_features[link_idx, 5] = 0\n",
        "\n",
        "    def increase_gdp(self, agent_id):\n",
        "        self.node_features[agent_id, 0] += 0.2 * self.node_features[agent_id, 0] * (random.random() + 0.5)\n",
        "\n",
        "    def decrease_gdp(self, agent_id):\n",
        "        self.node_features[agent_id, 0] -= 0.2 * self.node_features[agent_id, 0] * (random.random() + 0.5)\n",
        "\n",
        "    def increase_pop(self, agent_id):\n",
        "        self.node_features[agent_id, 1] += 0.2 * self.node_features[agent_id, 1] * (random.random() + 0.5)\n",
        "\n",
        "    def decrease_pop(self, agent_id):\n",
        "        self.node_features[agent_id, 1] -= 0.2 * self.node_features[agent_id, 1] * (random.random() + 0.5)\n",
        "\n",
        "    def step(self):\n",
        "        # gdp and pop fluctuations\n",
        "        self.node_features[:, 0] += 0.05 * self.node_features[:, 0] * (torch.rand(self.num_countries, dtype=torch.float32) - 0.5)\n",
        "        self.node_features[:, 1] += 0.05 * self.node_features[:, 1] * (torch.rand(self.num_countries, dtype=torch.float32) - 0.5)\n",
        "\n",
        "        # colonized countries can flip to having a common language\n",
        "        one_feat_shape = self.edge_features[:, 2].shape\n",
        "        self.edge_features[:, 2] = torch.min(torch.ones(one_feat_shape), self.edge_features[:, 2] + (self.edge_features[:, 5] * (torch.rand(one_feat_shape) > 0.9)))\n",
        "\n",
        "        # sea distance can shorten\n",
        "        self.edge_features[:, 4] = torch.min(self.edge_features[:, 0] * 1.5, self.edge_features[:, 4] * torch.max(torch.ones(one_feat_shape), 0.8 + torch.rand(one_feat_shape) * 20))\n",
        "\n",
        "        data = geo.data.Data(x=self.node_features, edge_index=self.edge_indexes, edge_attr=self.edge_features)\n",
        "        self.node_demo = self.env_model(data)\n",
        "\n",
        "        self.create_normed_state()\n",
        "        \n",
        "    def create_normed_state(self):\n",
        "        self.norm_state = geo.data.Data(x = (self.node_features.clone() - self.norm_stats[\"x_mean\"][:2]) / self.norm_stats[\"x_std\"][:2],\n",
        "                                        edge_index = self.edge_indexes,\n",
        "                                        edge_attr = (self.edge_features.clone() - self.norm_stats[\"attr_mean\"]) / self.norm_stats[\"attr_std\"])\n",
        "\n",
        "    def get_reward(self, agent_id):\n",
        "        agent_cluster = []\n",
        "        for cluster in self.clusters:\n",
        "            if agent_id in cluster:\n",
        "                agent_cluster = cluster\n",
        "\n",
        "        reward = 0\n",
        "        for country_idx in range(self.num_countries):\n",
        "            demo = self.node_demo[country_idx]\n",
        "            if country_idx == agent_id:\n",
        "                reward += 2 * demo\n",
        "            elif country_idx in agent_cluster:\n",
        "                reward += demo\n",
        "            else:\n",
        "                reward -= demo\n",
        "                \n",
        "        return reward\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrmC8VlOJ7K4"
      },
      "source": [
        "class RecurGraphAgent(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_node_output_features, num_graph_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # initial trainable hidden state for lstm\n",
        "        self.lstm_h_s = torch.nn.Linear(1, lstm_layer_size)\n",
        "        self.lstm_c_s = torch.nn.Linear(1, lstm_layer_size)\n",
        "\n",
        "        # graph pooling layer\n",
        "        self.pool = geo.nn.GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(lstm_layer_size, 2*lstm_layer_size), torch.nn.ReLU(), torch.nn.Linear(2*lstm_layer_size, 1)))\n",
        "\n",
        "        # final graph output\n",
        "        self.final_graph_linear = torch.nn.Linear(lstm_layer_size, num_graph_output_features)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_node_linear = torch.nn.Linear(lstm_layer_size, num_node_output_features)\n",
        "\n",
        "    def reset(self, initial):\n",
        "        self.new_seq = True\n",
        "        self.initial = initial\n",
        "\n",
        "    def forward(self, input, step=True):\n",
        "        if step:\n",
        "            # create graph representation\n",
        "            graph_step = torch.nn.functional.relu(self.conv(input.x, input.edge_index, input.edge_attr))\n",
        "\n",
        "            # recurrent stage\n",
        "            # initial state of lstm is representation of target prior to this sequence\n",
        "            if self.new_seq:\n",
        "                self.new_seq = False\n",
        "                self.hs = self.lstm_h_s(self.initial).unsqueeze(0)\n",
        "                self.cs = self.lstm_c_s(self.initial).unsqueeze(0)\n",
        "\n",
        "            lstm_output, (self.hs, self.cs) = self.lstm(graph_step.unsqueeze(0), (self.hs, self.cs))\n",
        "\n",
        "        else:\n",
        "            initial, sequence = input.initial, input.sequence\n",
        "            \n",
        "            # create graph representation\n",
        "            graph_collection = []\n",
        "            for idx in range(len(sequence)):\n",
        "                x, edge_index, edge_attr = sequence[idx].x, sequence[idx].edge_index, sequence[idx].edge_attr\n",
        "                graph_step = torch.nn.functional.relu(self.conv(x, edge_index, edge_attr))\n",
        "                graph_collection.append(graph_step)\n",
        "            # provide graph representations as sequence to lstm\n",
        "            graph_series = torch.stack(graph_collection)\n",
        "\n",
        "            # recurrent stage\n",
        "            # initial state of lstm is representation of target prior to this sequence\n",
        "            lstm_output, _ = self.lstm(graph_series, (self.lstm_h_s(initial).unsqueeze(0), self.lstm_c_s(initial).unsqueeze(0)))\n",
        "\n",
        "        # get last outputi\n",
        "        lstm_final_output = lstm_output[-1, :, :]\n",
        "\n",
        "        graph_pool = self.pool(lstm_final_output, input.batch)\n",
        "        final_graph = self.final_graph_linear(graph_pool)\n",
        "        graph_flattened = final_graph.view(-1)\n",
        "\n",
        "        final_node = self.final_node_linear(lstm_final_output)\n",
        "        node_flattened = final_node.view(-1)\n",
        "\n",
        "        return torch.nn.functional.softmax(node_flattened), torch.nn.functional.softmax(graph_flattened)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk2DceslIkWD"
      },
      "source": [
        "We want to be able to allow multiple actions per turn. We have previously defined a model with branching outputs. We will consider the predicted Q value to be the sum of the model outputs that are chosen as actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av0VHgnn1HTr"
      },
      "source": [
        "class NationAgent():\n",
        "    def __init__(self, agent_id, num_countries, replay_capacity, num_node_actions, num_global_actions, device):\n",
        "        # more node features because we will add indicator of self country and ally countries\n",
        "        num_node_features, num_edge_features = 4, 7\n",
        "\n",
        "        # create two DQNs for stable learning\n",
        "        self.policy_net = RecurGraphAgent(num_node_features, num_edge_features, num_node_actions, num_global_actions).to(device)\n",
        "        self.target_net = RecurGraphAgent(num_node_features, num_edge_features, num_node_actions, num_global_actions).to(device)\n",
        "        self.optimizer = torch.optim.RMSprop(self.policy_net.parameters())\n",
        "\n",
        "        self.memory = ReplayMemory(replay_capacity)\n",
        "\n",
        "        # ensure they match\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.agent_id = agent_id\n",
        "        self.num_countries = num_countries\n",
        "        self.num_node_actions = num_node_actions\n",
        "        self.num_global_actions = num_global_actions\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def reset(self, state_dict, ally_countries, demo_initial):\n",
        "        self.policy_net.load_state_dict(state_dict)\n",
        "        self.target_net.load_state_dict(state_dict)\n",
        "\n",
        "        self.policy_net.reset(demo_initial)\n",
        "        self.target_net.reset(demo_initial)\n",
        "\n",
        "        # create node data with features for self and ally countries\n",
        "        # using -0.1 and 0.9 as approximation of normalization\n",
        "        self.node_features = -0.1 * torch.ones((self.num_countries, 4), dtype=torch.float32)\n",
        "        self.node_features[self.agent_id, 2] = 0.9\n",
        "        for ally_idx in ally_countries:\n",
        "            self.node_features[ally_idx, 3] = 0.9\n",
        "\n",
        "        batch = torch.zeros(self.num_countries, dtype=torch.long, device=self.device)\n",
        "        self.memory.reset(demo_initial, batch)\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.policy_net.state_dict()\n",
        "\n",
        "    def select_action(self, env_state, eps_threshold):\n",
        "        # add in country specific state\n",
        "        self.node_features[:, :2] = env_state.x[:,:2]\n",
        "\n",
        "        state = geo.data.Data(x=self.node_features,\n",
        "                              edge_index=env_state.edge_index.clone(),\n",
        "                              edge_attr=env_state.edge_attr.clone())\n",
        "        \n",
        "        state.batch = torch.zeros(self.num_countries, dtype=torch.long, device=self.device)\n",
        "\n",
        "        sample = random.random()\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                foreign_output, domestic_output = self.policy_net(state)\n",
        "                foreign_action = torch.argmax(foreign_output)\n",
        "                domestic_action = torch.argmax(domestic_output)\n",
        "                return foreign_action, domestic_action\n",
        "        else:\n",
        "            return torch.tensor(random.randrange(self.num_node_actions), device=self.device, dtype=torch.long), torch.tensor(random.randrange(self.num_global_actions), device=self.device, dtype=torch.long)\n",
        "\n",
        "    def add_transition(self, transition):\n",
        "        # add in country specific state\n",
        "        self.node_features[:, :2] = transition.state.x[:,:2]\n",
        "        transition.state.x = self.node_features\n",
        "\n",
        "        self.node_features[:, :2] = transition.next_state.x[:,:2]\n",
        "        transition.next_state.x = self.node_features\n",
        "\n",
        "        self.memory.push(transition)\n",
        "\n",
        "    def optimize(self):\n",
        "        # single transition because i haven't worked out how to make batches work with net yet\n",
        "        transition = self.memory.sample()\n",
        "\n",
        "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "        # columns of actions taken. These are the actions which would've been taken\n",
        "        # for each batch state according to policy_net\n",
        "        foreign_output, domestic_output = self.policy_net(transition.state, step=False)\n",
        "        state_action_values = foreign_output[transition.action.foreign] + domestic_output[transition.action.domestic]\n",
        "\n",
        "        # Compute V(s_{t+1}) for all next states.\n",
        "        # Expected values of actions for non_final_next_states are computed based\n",
        "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "        # this environment never technically ends, so we shouldn't expect the agent to predict a final step of rewards\n",
        "        \n",
        "        foreign_output, domestic_output = self.target_net(transition.next_state, step=False)\n",
        "        next_state_values = foreign_output.max().detach() + domestic_output.max().detach()\n",
        "        # Compute the expected Q values\n",
        "        expected_state_action_values = (next_state_values * GAMMA) + transition.reward\n",
        "\n",
        "        # Compute Huber loss\n",
        "        loss = torch.nn.functional.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh1peXrZy4XT"
      },
      "source": [
        "class InternationalAgentCollection():\n",
        "    def __init__(self, num_countries, replay_capacity, num_node_actions, num_global_actions, device):\n",
        "        self.device = device\n",
        "\n",
        "        # create agents\n",
        "        self.agents = []\n",
        "        for agent_id in range(num_countries):\n",
        "            new_agent = NationAgent(agent_id, num_countries, replay_capacity, num_node_actions, num_global_actions, device)\n",
        "            self.agents.append(new_agent)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.agents[idx]\n",
        "\n",
        "    def reset(self, ally_groups, demo_initial):\n",
        "        new_state_dict = self.get_state()\n",
        "\n",
        "        # and then apply averaged state dict to agents\n",
        "        for agent_idx, agent in enumerate(self.agents):\n",
        "            agent_ally_group = []\n",
        "            for ally_group in ally_groups:\n",
        "                if agent_idx in ally_group:\n",
        "                    agent_ally_group = ally_group\n",
        "            # reset each individual agent\n",
        "            agent.reset(new_state_dict, agent_ally_group, demo_initial)\n",
        "\n",
        "    def get_state(self):\n",
        "        # get state dict from all agents\n",
        "        all_agent_states = []\n",
        "        for agent in self.agents:\n",
        "            all_agent_states.append(agent.get_state())\n",
        "\n",
        "        # average them\n",
        "        new_state_dict = all_agent_states[0]\n",
        "        for key in new_state_dict:\n",
        "            for idx in range(1, len(all_agent_states)):\n",
        "                new_state_dict[key] += all_agent_states[idx][key]\n",
        "            new_state_dict[key] = new_state_dict[key] / len(all_agent_states)\n",
        "\n",
        "        return new_state_dict\n",
        "\n",
        "    def select_actions(self, state, eps_threshold):\n",
        "        agent_actions = []\n",
        "        for agent in self.agents:\n",
        "            action = agent.select_action(state, eps_threshold)\n",
        "            agent_actions.append(action)\n",
        "        return agent_actions\n",
        "\n",
        "    def optimize(self):\n",
        "        for agent in self.agents:\n",
        "            agent.optimize()\n",
        "\n",
        "            "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekP7B3gH529M"
      },
      "source": [
        "Function mapping action index choices to actions in the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e311ws9wXNm_"
      },
      "source": [
        "def apply_actions(actions, env):\n",
        "    for agent_idx in range(len(actions)):\n",
        "        foreign_action, domestic_action = actions[agent_idx]\n",
        "        foreign_target_idx = math.floor(foreign_action / env.num_foreign_actions)\n",
        "        foreign_target_action = foreign_action % env.num_foreign_actions\n",
        "\n",
        "        if foreign_target_action == 0:\n",
        "            env.establish_trade(agent_idx, foreign_target_idx)\n",
        "        elif foreign_target_action == 1:\n",
        "            env.increase_imports(agent_idx, foreign_target_idx)\n",
        "        elif foreign_target_action == 2:\n",
        "            env.decrease_imports(agent_idx, foreign_target_idx)\n",
        "        elif foreign_target_action == 3:\n",
        "            env.colonize(agent_idx, foreign_target_idx)\n",
        "        elif foreign_target_action == 4:\n",
        "            env.decolonize(agent_idx, foreign_target_idx)\n",
        "\n",
        "        if domestic_action == 0:\n",
        "            env.increase_gdp(agent_idx)\n",
        "        elif domestic_action == 1:\n",
        "            env.decrease_gdp(agent_idx)\n",
        "        elif domestic_action == 2:\n",
        "            env.increase_pop(agent_idx)\n",
        "        elif domestic_action == 3:\n",
        "            env.decrease_pop(agent_idx)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn8WL9tE58Ss"
      },
      "source": [
        "Defining constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_maLVjcwkb6"
      },
      "source": [
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 50\n",
        "NUM_EPISODES = 100\n",
        "REPLAY_CAPACITY = 20\n",
        "\n",
        "NUM_COUNTRIES = 100\n",
        "NUM_YEARS_PER_ROUND = 200"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm7Q1Ldx9jyY"
      },
      "source": [
        "Main training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RdIvoL_CJsh",
        "outputId": "335a2464-f0c3-4c77-82dc-5d65e02a8795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = NationEnvironment(NUM_COUNTRIES, device)\n",
        "agents = InternationalAgentCollection(NUM_COUNTRIES, REPLAY_CAPACITY, env.num_foreign_actions, env.num_domestic_actions, device)\n",
        "\n",
        "for i_episode in range(NUM_EPISODES):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    agents.reset(env.clusters, env.norm_initial_demo)\n",
        "\n",
        "    # reward stats\n",
        "    reward_mean = 0\n",
        "    reward_var = 0\n",
        "\n",
        "    with tqdm.tqdm(range(NUM_YEARS_PER_ROUND)) as years:\n",
        "        for year in years:\n",
        "            years.set_postfix(str=\"Reward Mean: %i, Reward Var: %i\" % (reward_mean, reward_var))\n",
        "\n",
        "            # get state at start of round\n",
        "            state = env.norm_state\n",
        "\n",
        "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "                            math.exp(-1. * i_episode / EPS_DECAY)\n",
        "\n",
        "            # Select and perform an action\n",
        "            actions = agents.select_actions(env.norm_state, eps_threshold)\n",
        "            apply_actions(actions, env)\n",
        "\n",
        "            # let environment take step\n",
        "            env.step()\n",
        "\n",
        "            # Observe new state\n",
        "            next_state = env.norm_state\n",
        "\n",
        "            rewards = torch.zeros(NUM_COUNTRIES)\n",
        "            # Store the transition in memory\n",
        "            for agent_id in range(NUM_COUNTRIES):\n",
        "                # get the reward\n",
        "                reward = env.get_reward(agent_id)\n",
        "                rewards[agent_id] = reward\n",
        "                action = Action(foreign = actions[agent_id][0],\n",
        "                                domestic = actions[agent_id][1])\n",
        "                transition = Transition(state = state,\n",
        "                                        action = action,\n",
        "                                        next_state = next_state,\n",
        "                                        reward = reward)\n",
        "                agents[agent_id].add_transition(transition)\n",
        "\n",
        "            reward_mean = torch.mean(rewards)\n",
        "            reward_var = torch.var(rewards)\n",
        "\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            agents.optimize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 30/200 [03:50<35:50, 12.65s/it, str=Reward Mean: -96, Reward Var: 427]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj0NUs3XutNm"
      },
      "source": [
        "torch.save(agents.get_state(), os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_agent.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbjwwBCpv6KH"
      },
      "source": [
        "Things To Try Next:\n",
        "\n",
        "*   Weights only shared in cluster\n",
        "*   Dueling DQN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e99Nj-fcAKs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}